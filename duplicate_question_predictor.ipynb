{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-20T11:55:45.749519Z","iopub.execute_input":"2023-01-20T11:55:45.749953Z","iopub.status.idle":"2023-01-20T11:55:46.130419Z","shell.execute_reply.started":"2023-01-20T11:55:45.749863Z","shell.execute_reply":"2023-01-20T11:55:46.129518Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install Distance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Problem Statement\n**Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.**","metadata":{}},{"cell_type":"markdown","source":"## Data Import and Understand Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/quora-question-pairs/train.csv.zip\")\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.sample(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are 404290 entries in the data with 6 columns","metadata":{}},{"cell_type":"code","source":"# missing values\ndf.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are null values in the entries in the data. But the number is very small as compared to the data.","metadata":{}},{"cell_type":"code","source":"# duplicate rows\ndf.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There is no duplicate rows in the data","metadata":{}},{"cell_type":"code","source":"# Distribution of duplicate and non-duplicate questions\n\nprint(df['is_duplicate'].value_counts())\nprint((df['is_duplicate'].value_counts()/df['is_duplicate'].count())*100)\ndf['is_duplicate'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are 149263 i.e. 36% duplicate question entries in the data and 255027 i.e. 63% non duplicate entries. ","metadata":{}},{"cell_type":"code","source":"# Repeated questions\n\nqid = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nprint('Total Number of questions',qid.shape[0])\nprint('Number of unique questions',np.unique(qid).shape[0])\nx = qid.value_counts()>1\nprint('Number of questions getting repeated',x[x].shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Repeated questions histogram\n\nplt.hist(qid.value_counts().values,bins=160)\nplt.yscale('log')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying with Bag of Words vectorization method with Randomforest and evealuating the results","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = df.sample(30000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ques_df = new_df[['question1','question2']]\nques_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge texts\nquestions = list(ques_df['question1']) + list(ques_df['question2'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(max_features=4000)\nq1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)\ntemp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df = pd.concat([temp_df1, temp_df2], axis=1)\ntemp_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df['is_duplicate'] = new_df['is_duplicate']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df.sample(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(temp_df.iloc[:,0:-1].values,temp_df.iloc[:,-1].values,test_size=0.2,random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from xgboost import XGBClassifier\n# xgb = XGBClassifier()\n# xgb.fit(X_train,y_train)\n# y_pred = xgb.predict(X_test)\n# accuracy_score(y_test,y_pred)\n\n# with 30000 dataset limit the accuracy we get is 0.7408","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Trying with Bag of Words and some basic new feature creation and training with Randomforest and evealuating the results","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')\ndf.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df = df.sample(100000,random_state=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.dropna(inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Distribution of duplicate and non-duplicate questions\n\nprint(new_df['is_duplicate'].value_counts())\nprint((new_df['is_duplicate'].value_counts()/new_df['is_duplicate'].count())*100)\nnew_df['is_duplicate'].value_counts().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Repeated questions\n\nqid = pd.Series(new_df['qid1'].tolist() + new_df['qid2'].tolist())\nprint('Number of unique questions',np.unique(qid).shape[0])\nx = qid.value_counts()>1\nprint('Number of questions getting repeated',x[x].shape[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Repeated questions histogram\n\nplt.hist(qid.value_counts().values,bins=160)\nplt.yscale('log')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Above analysis tells us that the sample of size 30000 is the perfect representation of the whole dataset","metadata":{}},{"cell_type":"markdown","source":"### # Feature Engineering: Adding 7 new features as follows\n* q1_len: length of question 1\n* q2_len: length of question 2\n* q1_num_words: num of words in question 1\n* q2_num_words: num of words in question 2\n* word_common: words common in q1 and q2\n* word_total: total common words in q1 and q2\n* word_share: word_common/word_total","metadata":{}},{"cell_type":"code","source":"new_df['q1_len'] = new_df['question1'].str.len() \nnew_df['q2_len'] = new_df['question2'].str.len()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\nnew_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))\nnew_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def common_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return len(w1 & w2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['word_common'] = new_df.apply(common_words, axis=1)\nnew_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def total_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return (len(w1) + len(w2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['word_total'] = new_df.apply(total_words, axis=1)\nnew_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)\nnew_df.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysing new features","metadata":{}},{"cell_type":"code","source":"sns.displot(new_df['q1_len'])\nprint('minimum characters',new_df['q1_len'].min())\nprint('maximum characters',new_df['q1_len'].max())\nprint('average num of characters',int(new_df['q1_len'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(new_df['q2_len'])\nprint('minimum characters',new_df['q2_len'].min())\nprint('maximum characters',new_df['q2_len'].max())\nprint('average num of characters',int(new_df['q2_len'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(new_df['q1_num_words'])\nprint('minimum words',new_df['q1_num_words'].min())\nprint('maximum words',new_df['q1_num_words'].max())\nprint('average num of words',int(new_df['q1_num_words'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(new_df['q2_num_words'])\nprint('minimum words',new_df['q2_num_words'].min())\nprint('maximum words',new_df['q2_num_words'].max())\nprint('average num of words',int(new_df['q2_num_words'].mean()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# common words\nsns.distplot(new_df[new_df['is_duplicate'] == 0]['word_common'],label='non duplicate')\nsns.distplot(new_df[new_df['is_duplicate'] == 1]['word_common'],label='duplicate')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# total words\nsns.distplot(new_df[new_df['is_duplicate'] == 0]['word_total'],label='non duplicate')\nsns.distplot(new_df[new_df['is_duplicate'] == 1]['word_total'],label='duplicate')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word share\nsns.distplot(new_df[new_df['is_duplicate'] == 0]['word_share'],label='non duplicate')\nsns.distplot(new_df[new_df['is_duplicate'] == 1]['word_share'],label='duplicate')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ques_df = new_df[['question1','question2']]\nques_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = new_df.drop(columns=['id','qid1','qid2','question1','question2'])\nprint(final_df.shape)\nfinal_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# merge texts\nquestions = list(ques_df['question1']) + list(ques_df['question2'])\n\ncv = CountVectorizer(max_features=3000)\nq1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)\ntemp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)\ntemp_df = pd.concat([temp_df1, temp_df2], axis=1)\ntemp_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_df = pd.concat([final_df, temp_df], axis=1)\nprint(final_df.shape)\nfinal_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size=0.2,random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BOW with Preprocessing and Advance Feature Engineering ","metadata":{}},{"cell_type":"code","source":"import re\nfrom bs4 import BeautifulSoup","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:55:58.657236Z","iopub.execute_input":"2023-01-20T11:55:58.657602Z","iopub.status.idle":"2023-01-20T11:55:58.752364Z","shell.execute_reply.started":"2023-01-20T11:55:58.657573Z","shell.execute_reply":"2023-01-20T11:55:58.750785Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')\nnew_df = df.sample(100000,random_state=2)\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:56:11.429858Z","iopub.execute_input":"2023-01-20T11:56:11.431204Z","iopub.status.idle":"2023-01-20T11:56:12.688274Z","shell.execute_reply.started":"2023-01-20T11:56:11.431160Z","shell.execute_reply":"2023-01-20T11:56:12.687647Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n398782  398782  496695  532029   \n115086  115086  187729  187730   \n327711  327711  454161  454162   \n367788  367788  498109  491396   \n151235  151235  237843   50930   \n\n                                                question1  \\\n398782  What is the best marketing automation tool for...   \n115086  I am poor but I want to invest. What should I do?   \n327711  I am from India and live abroad. I met a guy f...   \n367788  Why do so many people in the U.S. hate the sou...   \n151235                Consequences of Bhopal gas tragedy?   \n\n                                                question2  is_duplicate  \n398782  What is the best marketing automation tool for...             1  \n115086  I am quite poor and I want to be very rich. Wh...             0  \n327711  T.I.E.T to Thapar University to Thapar Univers...             0  \n367788  My boyfriend doesnt feel guilty when he hurts ...             0  \n151235  What was the reason behind the Bhopal gas trag...             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>398782</td>\n      <td>496695</td>\n      <td>532029</td>\n      <td>What is the best marketing automation tool for...</td>\n      <td>What is the best marketing automation tool for...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>115086</td>\n      <td>187729</td>\n      <td>187730</td>\n      <td>I am poor but I want to invest. What should I do?</td>\n      <td>I am quite poor and I want to be very rich. Wh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>327711</td>\n      <td>454161</td>\n      <td>454162</td>\n      <td>I am from India and live abroad. I met a guy f...</td>\n      <td>T.I.E.T to Thapar University to Thapar Univers...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>367788</td>\n      <td>498109</td>\n      <td>491396</td>\n      <td>Why do so many people in the U.S. hate the sou...</td>\n      <td>My boyfriend doesnt feel guilty when he hurts ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>151235</td>\n      <td>237843</td>\n      <td>50930</td>\n      <td>Consequences of Bhopal gas tragedy?</td>\n      <td>What was the reason behind the Bhopal gas trag...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess(q):\n    \n    # convert to lower case\n    q = str(q).lower().strip()\n    \n    # Replace certain special characters with their string equivalents\n    q = q.replace('%', ' percent')\n    q = q.replace('$', ' dollar ')\n    q = q.replace('₹', ' rupee ')\n    q = q.replace('€', ' euro ')\n    q = q.replace('@', ' at ')\n    \n    # The pattern '[math]' appears around 900 times in the whole dataset.\n    q = q.replace('[math]', '')\n    \n    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n    q = q.replace(',000,000,000 ', 'b ')\n    q = q.replace(',000,000 ', 'm ')\n    q = q.replace(',000 ', 'k ')\n    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n    \n    # Decontracting words\n    # https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n    # https://stackoverflow.com/a/19794953\n    contractions = { \n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"can not\",\n    \"can't've\": \"can not have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n    }\n\n    q_decontracted = []\n\n    for word in q.split():\n        if word in contractions:\n            word = contractions[word]\n\n        q_decontracted.append(word)\n\n    q = ' '.join(q_decontracted)\n    q = q.replace(\"'ve\", \" have\")\n    q = q.replace(\"n't\", \" not\")\n    q = q.replace(\"'re\", \" are\")\n    q = q.replace(\"'ll\", \" will\")\n    \n    # Removing HTML tags\n    q = BeautifulSoup(q)\n    q = q.get_text()\n    \n    # Remove punctuations\n    pattern = re.compile('\\W')\n    q = re.sub(pattern, ' ', q).strip()\n\n    \n    return q\n\npreprocess(\"I've already! wasn't <b>done</b>?\")","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:56:18.015606Z","iopub.execute_input":"2023-01-20T11:56:18.016719Z","iopub.status.idle":"2023-01-20T11:56:18.041917Z","shell.execute_reply.started":"2023-01-20T11:56:18.016685Z","shell.execute_reply":"2023-01-20T11:56:18.040884Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'i have already  was not done'"},"metadata":{}}]},{"cell_type":"code","source":"new_df['question1'] = new_df['question1'].apply(preprocess)\nnew_df['question2'] = new_df['question2'].apply(preprocess)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:56:21.279336Z","iopub.execute_input":"2023-01-20T11:56:21.279932Z","iopub.status.idle":"2023-01-20T11:57:08.080372Z","shell.execute_reply.started":"2023-01-20T11:56:21.279890Z","shell.execute_reply":"2023-01-20T11:57:08.079484Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/bs4/__init__.py:439: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  MarkupResemblesLocatorWarning\n","output_type":"stream"}]},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering: Creating basic features created above","metadata":{}},{"cell_type":"code","source":"new_df['q1_len'] = new_df['question1'].str.len() \nnew_df['q2_len'] = new_df['question2'].str.len()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:21.669573Z","iopub.execute_input":"2023-01-20T11:57:21.669948Z","iopub.status.idle":"2023-01-20T11:57:21.780115Z","shell.execute_reply.started":"2023-01-20T11:57:21.669919Z","shell.execute_reply":"2023-01-20T11:57:21.779210Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"new_df['q1_num_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\nnew_df['q2_num_words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))\nnew_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:24.421242Z","iopub.execute_input":"2023-01-20T11:57:24.421558Z","iopub.status.idle":"2023-01-20T11:57:24.624742Z","shell.execute_reply.started":"2023-01-20T11:57:24.421533Z","shell.execute_reply":"2023-01-20T11:57:24.623683Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n398782  398782  496695  532029   \n115086  115086  187729  187730   \n\n                                                question1  \\\n398782  what is the best marketing automation tool for...   \n115086   i am poor but i want to invest  what should i do   \n\n                                                question2  is_duplicate  \\\n398782  what is the best marketing automation tool for...             1   \n115086  i am quite poor and i want to be very rich  wh...             0   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  \n398782      75      76            13            13  \n115086      48      56            13            16  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>398782</td>\n      <td>496695</td>\n      <td>532029</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>1</td>\n      <td>75</td>\n      <td>76</td>\n      <td>13</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>115086</td>\n      <td>187729</td>\n      <td>187730</td>\n      <td>i am poor but i want to invest  what should i do</td>\n      <td>i am quite poor and i want to be very rich  wh...</td>\n      <td>0</td>\n      <td>48</td>\n      <td>56</td>\n      <td>13</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def common_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return len(w1 & w2)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:27.309316Z","iopub.execute_input":"2023-01-20T11:57:27.309677Z","iopub.status.idle":"2023-01-20T11:57:27.316009Z","shell.execute_reply.started":"2023-01-20T11:57:27.309643Z","shell.execute_reply":"2023-01-20T11:57:27.314848Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"new_df['word_common'] = new_df.apply(common_words, axis=1)\nnew_df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:29.847713Z","iopub.execute_input":"2023-01-20T11:57:29.848077Z","iopub.status.idle":"2023-01-20T11:57:32.131340Z","shell.execute_reply.started":"2023-01-20T11:57:29.848045Z","shell.execute_reply":"2023-01-20T11:57:32.130479Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n395224  395224  528185  528186   \n132679  132679   34942   12852   \n\n                                                question1  \\\n395224  does the chinese communist party have any reli...   \n132679                 how can i earn money easily online   \n\n                                                question2  is_duplicate  \\\n395224  do many members in chinese communist party hat...             0   \n132679               how can i start to make money online             1   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  word_common  \n395224      59      51             9             9            4  \n132679      34      36             7             8            5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>395224</th>\n      <td>395224</td>\n      <td>528185</td>\n      <td>528186</td>\n      <td>does the chinese communist party have any reli...</td>\n      <td>do many members in chinese communist party hat...</td>\n      <td>0</td>\n      <td>59</td>\n      <td>51</td>\n      <td>9</td>\n      <td>9</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>132679</th>\n      <td>132679</td>\n      <td>34942</td>\n      <td>12852</td>\n      <td>how can i earn money easily online</td>\n      <td>how can i start to make money online</td>\n      <td>1</td>\n      <td>34</td>\n      <td>36</td>\n      <td>7</td>\n      <td>8</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def total_words(row):\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return (len(w1) + len(w2))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:35.066510Z","iopub.execute_input":"2023-01-20T11:57:35.066902Z","iopub.status.idle":"2023-01-20T11:57:35.072425Z","shell.execute_reply.started":"2023-01-20T11:57:35.066868Z","shell.execute_reply":"2023-01-20T11:57:35.071397Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"new_df['word_total'] = new_df.apply(total_words, axis=1)\nnew_df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:37.951301Z","iopub.execute_input":"2023-01-20T11:57:37.951625Z","iopub.status.idle":"2023-01-20T11:57:40.184086Z","shell.execute_reply.started":"2023-01-20T11:57:37.951599Z","shell.execute_reply":"2023-01-20T11:57:40.182778Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n97439    97439  162131  162132   \n382211  382211  514038  390538   \n\n                                                question1  \\\n97439   which is the best country to do a master s deg...   \n382211  how can darth vader breathe and talk at the sa...   \n\n                                                question2  is_duplicate  \\\n97439   which is best country for doing master of scie...             1   \n382211                          does darth vader get paid             0   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  word_common  word_total  \n97439       70      70            14            12            8          26  \n382211      53      25            11             5            2          16  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>97439</th>\n      <td>97439</td>\n      <td>162131</td>\n      <td>162132</td>\n      <td>which is the best country to do a master s deg...</td>\n      <td>which is best country for doing master of scie...</td>\n      <td>1</td>\n      <td>70</td>\n      <td>70</td>\n      <td>14</td>\n      <td>12</td>\n      <td>8</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>382211</th>\n      <td>382211</td>\n      <td>514038</td>\n      <td>390538</td>\n      <td>how can darth vader breathe and talk at the sa...</td>\n      <td>does darth vader get paid</td>\n      <td>0</td>\n      <td>53</td>\n      <td>25</td>\n      <td>11</td>\n      <td>5</td>\n      <td>2</td>\n      <td>16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_df['word_share'] = round(new_df['word_common']/new_df['word_total'],2)\nnew_df.sample(2)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:43.838053Z","iopub.execute_input":"2023-01-20T11:57:43.839190Z","iopub.status.idle":"2023-01-20T11:57:43.855593Z","shell.execute_reply.started":"2023-01-20T11:57:43.839126Z","shell.execute_reply":"2023-01-20T11:57:43.854553Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n334526  334526  461709  461710   \n130257  130257  209080  118958   \n\n                                                question1  \\\n334526  what s the most unprofessional thing a medical...   \n130257  what does it mean when the girlfriend says i a...   \n\n                                                question2  is_duplicate  \\\n334526  what is the most savage thing you have ever do...             0   \n130257  should i break up with my girlfriend for someo...             0   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  word_common  word_total  \\\n334526      65      56            12            12            6          24   \n130257      95      55            21            10            3          30   \n\n        word_share  \n334526        0.25  \n130257        0.10  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_total</th>\n      <th>word_share</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>334526</th>\n      <td>334526</td>\n      <td>461709</td>\n      <td>461710</td>\n      <td>what s the most unprofessional thing a medical...</td>\n      <td>what is the most savage thing you have ever do...</td>\n      <td>0</td>\n      <td>65</td>\n      <td>56</td>\n      <td>12</td>\n      <td>12</td>\n      <td>6</td>\n      <td>24</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>130257</th>\n      <td>130257</td>\n      <td>209080</td>\n      <td>118958</td>\n      <td>what does it mean when the girlfriend says i a...</td>\n      <td>should i break up with my girlfriend for someo...</td>\n      <td>0</td>\n      <td>95</td>\n      <td>55</td>\n      <td>21</td>\n      <td>10</td>\n      <td>3</td>\n      <td>30</td>\n      <td>0.10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Advance Feature engineering\n1. Token Features\n    * cwc_min: This is the ratio of the number of common words to the length of the smaller question\n    * cwc_max: This is the ratio of the number of common words to the length of the larger question\n    * csc_min: This is the ratio of the number of common stop words to the smaller stop word count among the two questions\n    * csc_max: This is the ratio of the number of common stop words to the larger stop word count among the two questions\n    * ctc_min: This is the ratio of the number of common tokens to the smaller token count among the two questions\n    * ctc_max: This is the ratio of the number of common tokens to the larger token count among the two questions\n    * last_word_eq: 1 if the last word in the two questions is same, 0 otherwise\n    * first_word_eq: 1 if the first word in the two questions is same, 0 otherwise\n2. Length Based Features\n    * mean_len: Mean of the length of the two questions (number of words)\n    * abs_len_diff: Absolute difference between the length of the two questions (number of words)\n    * longest_substr_ratio: Ratio of the length of the longest substring among the two questions to the length of the smaller question\n3. Fuzzy Features\n    * fuzz_ratio: fuzz_ratio score from fuzzywuzzy\n    * fuzz_partial_ratio: fuzz_partial_ratio from fuzzywuzzy\n    * token_sort_ratio: token_sort_ratio from fuzzywuzzy\n    * token_set_ratio: token_set_ratio from fuzzywuzzy ","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\ndef fetch_token_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    SAFE_DIV = 0.0001 \n\n    STOP_WORDS = stopwords.words(\"english\")\n    \n    token_features = [0.0]*8\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    return token_features","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:50.416839Z","iopub.execute_input":"2023-01-20T11:57:50.417196Z","iopub.status.idle":"2023-01-20T11:57:50.626881Z","shell.execute_reply.started":"2023-01-20T11:57:50.417169Z","shell.execute_reply":"2023-01-20T11:57:50.625958Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"token_features = new_df.apply(fetch_token_features, axis=1)\n\nnew_df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\nnew_df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\nnew_df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\nnew_df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\nnew_df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\nnew_df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\nnew_df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\nnew_df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:57:53.557606Z","iopub.execute_input":"2023-01-20T11:57:53.557993Z","iopub.status.idle":"2023-01-20T11:58:18.180910Z","shell.execute_reply.started":"2023-01-20T11:57:53.557965Z","shell.execute_reply":"2023-01-20T11:58:18.179834Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n398782  398782  496695  532029   \n115086  115086  187729  187730   \n327711  327711  454161  454162   \n367788  367788  498109  491396   \n151235  151235  237843   50930   \n\n                                                question1  \\\n398782  what is the best marketing automation tool for...   \n115086   i am poor but i want to invest  what should i do   \n327711  i am from india and live abroad  i met a guy f...   \n367788  why do so many people in the u s  hate the sou...   \n151235                 consequences of bhopal gas tragedy   \n\n                                                question2  is_duplicate  \\\n398782  what is the best marketing automation tool for...             1   \n115086  i am quite poor and i want to be very rich  wh...             0   \n327711  t i e t to thapar university to thapar univers...             0   \n367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n151235  what was the reason behind the bhopal gas tragedy             0   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  ...  word_total  \\\n398782      75      76            13            13  ...          26   \n115086      48      56            13            16  ...          24   \n327711     104     119            28            21  ...          38   \n367788      58     145            14            32  ...          34   \n151235      34      49             5             9  ...          13   \n\n        word_share   cwc_min   cwc_max   csc_min   csc_max   ctc_min  \\\n398782        0.46  0.874989  0.874989  0.999980  0.999980  0.923070   \n115086        0.33  0.666644  0.499988  0.714276  0.624992  0.583328   \n327711        0.11  0.000000  0.000000  0.428565  0.272725  0.149999   \n367788        0.03  0.000000  0.000000  0.000000  0.000000  0.000000   \n151235        0.23  0.749981  0.599988  0.000000  0.000000  0.599988   \n\n         ctc_max  last_word_eq  first_word_eq  \n398782  0.923070           1.0            1.0  \n115086  0.466664           1.0            1.0  \n327711  0.115384           0.0            0.0  \n367788  0.000000           0.0            0.0  \n151235  0.333330           1.0            0.0  \n\n[5 rows x 21 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>...</th>\n      <th>word_total</th>\n      <th>word_share</th>\n      <th>cwc_min</th>\n      <th>cwc_max</th>\n      <th>csc_min</th>\n      <th>csc_max</th>\n      <th>ctc_min</th>\n      <th>ctc_max</th>\n      <th>last_word_eq</th>\n      <th>first_word_eq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>398782</td>\n      <td>496695</td>\n      <td>532029</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>1</td>\n      <td>75</td>\n      <td>76</td>\n      <td>13</td>\n      <td>13</td>\n      <td>...</td>\n      <td>26</td>\n      <td>0.46</td>\n      <td>0.874989</td>\n      <td>0.874989</td>\n      <td>0.999980</td>\n      <td>0.999980</td>\n      <td>0.923070</td>\n      <td>0.923070</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>115086</td>\n      <td>187729</td>\n      <td>187730</td>\n      <td>i am poor but i want to invest  what should i do</td>\n      <td>i am quite poor and i want to be very rich  wh...</td>\n      <td>0</td>\n      <td>48</td>\n      <td>56</td>\n      <td>13</td>\n      <td>16</td>\n      <td>...</td>\n      <td>24</td>\n      <td>0.33</td>\n      <td>0.666644</td>\n      <td>0.499988</td>\n      <td>0.714276</td>\n      <td>0.624992</td>\n      <td>0.583328</td>\n      <td>0.466664</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>327711</td>\n      <td>454161</td>\n      <td>454162</td>\n      <td>i am from india and live abroad  i met a guy f...</td>\n      <td>t i e t to thapar university to thapar univers...</td>\n      <td>0</td>\n      <td>104</td>\n      <td>119</td>\n      <td>28</td>\n      <td>21</td>\n      <td>...</td>\n      <td>38</td>\n      <td>0.11</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.428565</td>\n      <td>0.272725</td>\n      <td>0.149999</td>\n      <td>0.115384</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>367788</td>\n      <td>498109</td>\n      <td>491396</td>\n      <td>why do so many people in the u s  hate the sou...</td>\n      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n      <td>0</td>\n      <td>58</td>\n      <td>145</td>\n      <td>14</td>\n      <td>32</td>\n      <td>...</td>\n      <td>34</td>\n      <td>0.03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>151235</td>\n      <td>237843</td>\n      <td>50930</td>\n      <td>consequences of bhopal gas tragedy</td>\n      <td>what was the reason behind the bhopal gas tragedy</td>\n      <td>0</td>\n      <td>34</td>\n      <td>49</td>\n      <td>5</td>\n      <td>9</td>\n      <td>...</td>\n      <td>13</td>\n      <td>0.23</td>\n      <td>0.749981</td>\n      <td>0.599988</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.599988</td>\n      <td>0.333330</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import distance\n\ndef fetch_length_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    length_features = [0.0]*3\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n    \n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n    \n    return length_features\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:58:25.004251Z","iopub.execute_input":"2023-01-20T11:58:25.005021Z","iopub.status.idle":"2023-01-20T11:58:25.016266Z","shell.execute_reply.started":"2023-01-20T11:58:25.004956Z","shell.execute_reply":"2023-01-20T11:58:25.015106Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"length_features = new_df.apply(fetch_length_features, axis=1)\n\nnew_df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\nnew_df['mean_len'] = list(map(lambda x: x[1], length_features))\nnew_df['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))\n\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T11:58:31.757469Z","iopub.execute_input":"2023-01-20T11:58:31.758394Z","iopub.status.idle":"2023-01-20T11:59:51.415320Z","shell.execute_reply.started":"2023-01-20T11:58:31.758361Z","shell.execute_reply":"2023-01-20T11:59:51.414460Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n398782  398782  496695  532029   \n115086  115086  187729  187730   \n327711  327711  454161  454162   \n367788  367788  498109  491396   \n151235  151235  237843   50930   \n\n                                                question1  \\\n398782  what is the best marketing automation tool for...   \n115086   i am poor but i want to invest  what should i do   \n327711  i am from india and live abroad  i met a guy f...   \n367788  why do so many people in the u s  hate the sou...   \n151235                 consequences of bhopal gas tragedy   \n\n                                                question2  is_duplicate  \\\n398782  what is the best marketing automation tool for...             1   \n115086  i am quite poor and i want to be very rich  wh...             0   \n327711  t i e t to thapar university to thapar univers...             0   \n367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n151235  what was the reason behind the bhopal gas tragedy             0   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  ...   cwc_max   csc_min  \\\n398782      75      76            13            13  ...  0.874989  0.999980   \n115086      48      56            13            16  ...  0.499988  0.714276   \n327711     104     119            28            21  ...  0.000000  0.428565   \n367788      58     145            14            32  ...  0.000000  0.000000   \n151235      34      49             5             9  ...  0.599988  0.000000   \n\n         csc_max   ctc_min   ctc_max  last_word_eq  first_word_eq  \\\n398782  0.999980  0.923070  0.923070           1.0            1.0   \n115086  0.624992  0.583328  0.466664           1.0            1.0   \n327711  0.272725  0.149999  0.115384           0.0            0.0   \n367788  0.000000  0.000000  0.000000           0.0            0.0   \n151235  0.000000  0.599988  0.333330           1.0            0.0   \n\n        abs_len_diff  mean_len  longest_substr_ratio  \n398782           0.0      13.0              0.855263  \n115086           3.0      13.5              0.224490  \n327711           6.0      23.0              0.047619  \n367788          17.0      21.5              0.050847  \n151235           4.0       7.0              0.542857  \n\n[5 rows x 24 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>...</th>\n      <th>cwc_max</th>\n      <th>csc_min</th>\n      <th>csc_max</th>\n      <th>ctc_min</th>\n      <th>ctc_max</th>\n      <th>last_word_eq</th>\n      <th>first_word_eq</th>\n      <th>abs_len_diff</th>\n      <th>mean_len</th>\n      <th>longest_substr_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>398782</td>\n      <td>496695</td>\n      <td>532029</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>1</td>\n      <td>75</td>\n      <td>76</td>\n      <td>13</td>\n      <td>13</td>\n      <td>...</td>\n      <td>0.874989</td>\n      <td>0.999980</td>\n      <td>0.999980</td>\n      <td>0.923070</td>\n      <td>0.923070</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>0.855263</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>115086</td>\n      <td>187729</td>\n      <td>187730</td>\n      <td>i am poor but i want to invest  what should i do</td>\n      <td>i am quite poor and i want to be very rich  wh...</td>\n      <td>0</td>\n      <td>48</td>\n      <td>56</td>\n      <td>13</td>\n      <td>16</td>\n      <td>...</td>\n      <td>0.499988</td>\n      <td>0.714276</td>\n      <td>0.624992</td>\n      <td>0.583328</td>\n      <td>0.466664</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13.5</td>\n      <td>0.224490</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>327711</td>\n      <td>454161</td>\n      <td>454162</td>\n      <td>i am from india and live abroad  i met a guy f...</td>\n      <td>t i e t to thapar university to thapar univers...</td>\n      <td>0</td>\n      <td>104</td>\n      <td>119</td>\n      <td>28</td>\n      <td>21</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.428565</td>\n      <td>0.272725</td>\n      <td>0.149999</td>\n      <td>0.115384</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>23.0</td>\n      <td>0.047619</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>367788</td>\n      <td>498109</td>\n      <td>491396</td>\n      <td>why do so many people in the u s  hate the sou...</td>\n      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n      <td>0</td>\n      <td>58</td>\n      <td>145</td>\n      <td>14</td>\n      <td>32</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>21.5</td>\n      <td>0.050847</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>151235</td>\n      <td>237843</td>\n      <td>50930</td>\n      <td>consequences of bhopal gas tragedy</td>\n      <td>what was the reason behind the bhopal gas tragedy</td>\n      <td>0</td>\n      <td>34</td>\n      <td>49</td>\n      <td>5</td>\n      <td>9</td>\n      <td>...</td>\n      <td>0.599988</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.599988</td>\n      <td>0.333330</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>7.0</td>\n      <td>0.542857</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 24 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Fuzzy Features\nfrom fuzzywuzzy import fuzz\n\ndef fetch_fuzzy_features(row):\n    \n    q1 = row['question1']\n    q2 = row['question2']\n    \n    fuzzy_features = [0.0]*4\n    \n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:00:03.019770Z","iopub.execute_input":"2023-01-20T12:00:03.020161Z","iopub.status.idle":"2023-01-20T12:00:03.043069Z","shell.execute_reply.started":"2023-01-20T12:00:03.020122Z","shell.execute_reply":"2023-01-20T12:00:03.042330Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"fuzzy_features = new_df.apply(fetch_fuzzy_features, axis=1)\n\n# Creating new feature columns for fuzzy features\nnew_df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\nnew_df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\nnew_df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\nnew_df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))\n\n\nprint(new_df.shape)\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:00:05.640504Z","iopub.execute_input":"2023-01-20T12:00:05.641347Z","iopub.status.idle":"2023-01-20T12:00:19.978841Z","shell.execute_reply.started":"2023-01-20T12:00:05.641313Z","shell.execute_reply":"2023-01-20T12:00:19.977935Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(100000, 28)\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"            id    qid1    qid2  \\\n398782  398782  496695  532029   \n115086  115086  187729  187730   \n327711  327711  454161  454162   \n367788  367788  498109  491396   \n151235  151235  237843   50930   \n\n                                                question1  \\\n398782  what is the best marketing automation tool for...   \n115086   i am poor but i want to invest  what should i do   \n327711  i am from india and live abroad  i met a guy f...   \n367788  why do so many people in the u s  hate the sou...   \n151235                 consequences of bhopal gas tragedy   \n\n                                                question2  is_duplicate  \\\n398782  what is the best marketing automation tool for...             1   \n115086  i am quite poor and i want to be very rich  wh...             0   \n327711  t i e t to thapar university to thapar univers...             0   \n367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n151235  what was the reason behind the bhopal gas tragedy             0   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  ...   ctc_max  \\\n398782      75      76            13            13  ...  0.923070   \n115086      48      56            13            16  ...  0.466664   \n327711     104     119            28            21  ...  0.115384   \n367788      58     145            14            32  ...  0.000000   \n151235      34      49             5             9  ...  0.333330   \n\n        last_word_eq  first_word_eq  abs_len_diff  mean_len  \\\n398782           1.0            1.0           0.0      13.0   \n115086           1.0            1.0           3.0      13.5   \n327711           0.0            0.0           6.0      23.0   \n367788           0.0            0.0          17.0      21.5   \n151235           1.0            0.0           4.0       7.0   \n\n        longest_substr_ratio  fuzz_ratio  fuzz_partial_ratio  \\\n398782              0.855263          99                  99   \n115086              0.224490          69                  67   \n327711              0.047619          42                  42   \n367788              0.050847          37                  50   \n151235              0.542857          60                  71   \n\n        token_sort_ratio  token_set_ratio  \n398782                99               99  \n115086                67               74  \n327711                39               46  \n367788                38               40  \n151235                48               69  \n\n[5 rows x 28 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>...</th>\n      <th>ctc_max</th>\n      <th>last_word_eq</th>\n      <th>first_word_eq</th>\n      <th>abs_len_diff</th>\n      <th>mean_len</th>\n      <th>longest_substr_ratio</th>\n      <th>fuzz_ratio</th>\n      <th>fuzz_partial_ratio</th>\n      <th>token_sort_ratio</th>\n      <th>token_set_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>398782</td>\n      <td>496695</td>\n      <td>532029</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>what is the best marketing automation tool for...</td>\n      <td>1</td>\n      <td>75</td>\n      <td>76</td>\n      <td>13</td>\n      <td>13</td>\n      <td>...</td>\n      <td>0.923070</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>0.855263</td>\n      <td>99</td>\n      <td>99</td>\n      <td>99</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>115086</td>\n      <td>187729</td>\n      <td>187730</td>\n      <td>i am poor but i want to invest  what should i do</td>\n      <td>i am quite poor and i want to be very rich  wh...</td>\n      <td>0</td>\n      <td>48</td>\n      <td>56</td>\n      <td>13</td>\n      <td>16</td>\n      <td>...</td>\n      <td>0.466664</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13.5</td>\n      <td>0.224490</td>\n      <td>69</td>\n      <td>67</td>\n      <td>67</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>327711</td>\n      <td>454161</td>\n      <td>454162</td>\n      <td>i am from india and live abroad  i met a guy f...</td>\n      <td>t i e t to thapar university to thapar univers...</td>\n      <td>0</td>\n      <td>104</td>\n      <td>119</td>\n      <td>28</td>\n      <td>21</td>\n      <td>...</td>\n      <td>0.115384</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>23.0</td>\n      <td>0.047619</td>\n      <td>42</td>\n      <td>42</td>\n      <td>39</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>367788</td>\n      <td>498109</td>\n      <td>491396</td>\n      <td>why do so many people in the u s  hate the sou...</td>\n      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n      <td>0</td>\n      <td>58</td>\n      <td>145</td>\n      <td>14</td>\n      <td>32</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>21.5</td>\n      <td>0.050847</td>\n      <td>37</td>\n      <td>50</td>\n      <td>38</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>151235</td>\n      <td>237843</td>\n      <td>50930</td>\n      <td>consequences of bhopal gas tragedy</td>\n      <td>what was the reason behind the bhopal gas tragedy</td>\n      <td>0</td>\n      <td>34</td>\n      <td>49</td>\n      <td>5</td>\n      <td>9</td>\n      <td>...</td>\n      <td>0.333330</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>7.0</td>\n      <td>0.542857</td>\n      <td>60</td>\n      <td>71</td>\n      <td>48</td>\n      <td>69</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Understand and analyse new features","metadata":{}},{"cell_type":"code","source":"sns.pairplot(new_df[['ctc_min', 'cwc_min', 'csc_min', 'is_duplicate']],hue='is_duplicate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['ctc_max', 'cwc_max', 'csc_max', 'is_duplicate']],hue='is_duplicate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['last_word_eq', 'first_word_eq', 'is_duplicate']],hue='is_duplicate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['mean_len', 'abs_len_diff','longest_substr_ratio', 'is_duplicate']],hue='is_duplicate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(new_df[['fuzz_ratio', 'fuzz_partial_ratio','token_sort_ratio','token_set_ratio', 'is_duplicate']],hue='is_duplicate')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using TSNE for Dimentionality reduction for 15 Features(Generated after cleaning the data) to 3 dimention\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nX = MinMaxScaler().fit_transform(new_df[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = new_df['is_duplicate'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\ntsne2d = TSNE(\n    n_components=2,\n    init='random', # pca\n    random_state=101,\n    method='barnes_hut',\n    n_iter=1000,\n    verbose=2,\n    angle=0.5\n).fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_df = pd.DataFrame({'x':tsne2d[:,0], 'y':tsne2d[:,1] ,'label':y})\n\n# draw the plot in appropriate place in the grid\nsns.lmplot(data=x_df, x='x', y='y', hue='label', fit_reg=False, size=8,palette=\"Set1\",markers=['s','o'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorization","metadata":{}},{"cell_type":"code","source":"ques_df = new_df[['question1','question2']]\nques_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:00:34.910222Z","iopub.execute_input":"2023-01-20T12:00:34.910562Z","iopub.status.idle":"2023-01-20T12:00:34.929532Z","shell.execute_reply.started":"2023-01-20T12:00:34.910534Z","shell.execute_reply":"2023-01-20T12:00:34.928558Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                                                question1  \\\n398782  what is the best marketing automation tool for...   \n115086   i am poor but i want to invest  what should i do   \n\n                                                question2  \n398782  what is the best marketing automation tool for...  \n115086  i am quite poor and i want to be very rich  wh...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1</th>\n      <th>question2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>what is the best marketing automation tool for...</td>\n      <td>what is the best marketing automation tool for...</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>i am poor but i want to invest  what should i do</td>\n      <td>i am quite poor and i want to be very rich  wh...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_df = new_df.drop(columns=['id','qid1','qid2','question1','question2'])\nprint(final_df.shape)\nfinal_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:00:38.408579Z","iopub.execute_input":"2023-01-20T12:00:38.408940Z","iopub.status.idle":"2023-01-20T12:00:38.434476Z","shell.execute_reply.started":"2023-01-20T12:00:38.408912Z","shell.execute_reply":"2023-01-20T12:00:38.433495Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"(100000, 23)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"        is_duplicate  q1_len  q2_len  q1_num_words  q2_num_words  word_common  \\\n398782             1      75      76            13            13           12   \n115086             0      48      56            13            16            8   \n327711             0     104     119            28            21            4   \n367788             0      58     145            14            32            1   \n151235             0      34      49             5             9            3   \n\n        word_total  word_share   cwc_min   cwc_max  ...   ctc_max  \\\n398782          26        0.46  0.874989  0.874989  ...  0.923070   \n115086          24        0.33  0.666644  0.499988  ...  0.466664   \n327711          38        0.11  0.000000  0.000000  ...  0.115384   \n367788          34        0.03  0.000000  0.000000  ...  0.000000   \n151235          13        0.23  0.749981  0.599988  ...  0.333330   \n\n        last_word_eq  first_word_eq  abs_len_diff  mean_len  \\\n398782           1.0            1.0           0.0      13.0   \n115086           1.0            1.0           3.0      13.5   \n327711           0.0            0.0           6.0      23.0   \n367788           0.0            0.0          17.0      21.5   \n151235           1.0            0.0           4.0       7.0   \n\n        longest_substr_ratio  fuzz_ratio  fuzz_partial_ratio  \\\n398782              0.855263          99                  99   \n115086              0.224490          69                  67   \n327711              0.047619          42                  42   \n367788              0.050847          37                  50   \n151235              0.542857          60                  71   \n\n        token_sort_ratio  token_set_ratio  \n398782                99               99  \n115086                67               74  \n327711                39               46  \n367788                38               40  \n151235                48               69  \n\n[5 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_total</th>\n      <th>word_share</th>\n      <th>cwc_min</th>\n      <th>cwc_max</th>\n      <th>...</th>\n      <th>ctc_max</th>\n      <th>last_word_eq</th>\n      <th>first_word_eq</th>\n      <th>abs_len_diff</th>\n      <th>mean_len</th>\n      <th>longest_substr_ratio</th>\n      <th>fuzz_ratio</th>\n      <th>fuzz_partial_ratio</th>\n      <th>token_sort_ratio</th>\n      <th>token_set_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>1</td>\n      <td>75</td>\n      <td>76</td>\n      <td>13</td>\n      <td>13</td>\n      <td>12</td>\n      <td>26</td>\n      <td>0.46</td>\n      <td>0.874989</td>\n      <td>0.874989</td>\n      <td>...</td>\n      <td>0.923070</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>13.0</td>\n      <td>0.855263</td>\n      <td>99</td>\n      <td>99</td>\n      <td>99</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>0</td>\n      <td>48</td>\n      <td>56</td>\n      <td>13</td>\n      <td>16</td>\n      <td>8</td>\n      <td>24</td>\n      <td>0.33</td>\n      <td>0.666644</td>\n      <td>0.499988</td>\n      <td>...</td>\n      <td>0.466664</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13.5</td>\n      <td>0.224490</td>\n      <td>69</td>\n      <td>67</td>\n      <td>67</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>0</td>\n      <td>104</td>\n      <td>119</td>\n      <td>28</td>\n      <td>21</td>\n      <td>4</td>\n      <td>38</td>\n      <td>0.11</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.115384</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>23.0</td>\n      <td>0.047619</td>\n      <td>42</td>\n      <td>42</td>\n      <td>39</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>0</td>\n      <td>58</td>\n      <td>145</td>\n      <td>14</td>\n      <td>32</td>\n      <td>1</td>\n      <td>34</td>\n      <td>0.03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>17.0</td>\n      <td>21.5</td>\n      <td>0.050847</td>\n      <td>37</td>\n      <td>50</td>\n      <td>38</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>0</td>\n      <td>34</td>\n      <td>49</td>\n      <td>5</td>\n      <td>9</td>\n      <td>3</td>\n      <td>13</td>\n      <td>0.23</td>\n      <td>0.749981</td>\n      <td>0.599988</td>\n      <td>...</td>\n      <td>0.333330</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>7.0</td>\n      <td>0.542857</td>\n      <td>60</td>\n      <td>71</td>\n      <td>48</td>\n      <td>69</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 23 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# merge texts\nquestions = list(ques_df['question1']) + list(ques_df['question2'])\n\ncv = CountVectorizer(max_features=3000)\nq1_arr, q2_arr = np.vsplit(cv.fit_transform(questions).toarray(),2)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:00:43.925216Z","iopub.execute_input":"2023-01-20T12:00:43.925554Z","iopub.status.idle":"2023-01-20T12:00:46.859905Z","shell.execute_reply.started":"2023-01-20T12:00:43.925526Z","shell.execute_reply":"2023-01-20T12:00:46.858947Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"temp_df1 = pd.DataFrame(q1_arr, index= ques_df.index)\ntemp_df2 = pd.DataFrame(q2_arr, index= ques_df.index)\ntemp_df = pd.concat([temp_df1, temp_df2], axis=1)\ntemp_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:00:49.749446Z","iopub.execute_input":"2023-01-20T12:00:49.749808Z","iopub.status.idle":"2023-01-20T12:00:52.717912Z","shell.execute_reply.started":"2023-01-20T12:00:49.749780Z","shell.execute_reply":"2023-01-20T12:00:52.716841Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(100000, 6000)"},"metadata":{}}]},{"cell_type":"code","source":"final_df = pd.concat([final_df, temp_df], axis=1)\nprint(final_df.shape)\nfinal_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:00:55.209788Z","iopub.execute_input":"2023-01-20T12:00:55.210116Z","iopub.status.idle":"2023-01-20T12:00:56.116335Z","shell.execute_reply.started":"2023-01-20T12:00:55.210089Z","shell.execute_reply":"2023-01-20T12:00:56.115532Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"(100000, 6023)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"        is_duplicate  q1_len  q2_len  q1_num_words  q2_num_words  word_common  \\\n398782             1      75      76            13            13           12   \n115086             0      48      56            13            16            8   \n327711             0     104     119            28            21            4   \n367788             0      58     145            14            32            1   \n151235             0      34      49             5             9            3   \n\n        word_total  word_share   cwc_min   cwc_max  ...  2990  2991  2992  \\\n398782          26        0.46  0.874989  0.874989  ...     0     0     0   \n115086          24        0.33  0.666644  0.499988  ...     0     0     0   \n327711          38        0.11  0.000000  0.000000  ...     0     0     0   \n367788          34        0.03  0.000000  0.000000  ...     0     0     1   \n151235          13        0.23  0.749981  0.599988  ...     0     0     0   \n\n        2993  2994  2995  2996  2997  2998  2999  \n398782     0     0     0     0     0     0     0  \n115086     0     0     0     0     0     0     0  \n327711     0     0     0     0     0     0     0  \n367788     0     0     0     0     0     0     0  \n151235     0     0     0     0     0     0     0  \n\n[5 rows x 6023 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_total</th>\n      <th>word_share</th>\n      <th>cwc_min</th>\n      <th>cwc_max</th>\n      <th>...</th>\n      <th>2990</th>\n      <th>2991</th>\n      <th>2992</th>\n      <th>2993</th>\n      <th>2994</th>\n      <th>2995</th>\n      <th>2996</th>\n      <th>2997</th>\n      <th>2998</th>\n      <th>2999</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>398782</th>\n      <td>1</td>\n      <td>75</td>\n      <td>76</td>\n      <td>13</td>\n      <td>13</td>\n      <td>12</td>\n      <td>26</td>\n      <td>0.46</td>\n      <td>0.874989</td>\n      <td>0.874989</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>115086</th>\n      <td>0</td>\n      <td>48</td>\n      <td>56</td>\n      <td>13</td>\n      <td>16</td>\n      <td>8</td>\n      <td>24</td>\n      <td>0.33</td>\n      <td>0.666644</td>\n      <td>0.499988</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>327711</th>\n      <td>0</td>\n      <td>104</td>\n      <td>119</td>\n      <td>28</td>\n      <td>21</td>\n      <td>4</td>\n      <td>38</td>\n      <td>0.11</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>367788</th>\n      <td>0</td>\n      <td>58</td>\n      <td>145</td>\n      <td>14</td>\n      <td>32</td>\n      <td>1</td>\n      <td>34</td>\n      <td>0.03</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>151235</th>\n      <td>0</td>\n      <td>34</td>\n      <td>49</td>\n      <td>5</td>\n      <td>9</td>\n      <td>3</td>\n      <td>13</td>\n      <td>0.23</td>\n      <td>0.749981</td>\n      <td>0.599988</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 6023 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size=0.2,random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:01:00.847816Z","iopub.execute_input":"2023-01-20T12:01:00.848182Z","iopub.status.idle":"2023-01-20T12:01:11.625274Z","shell.execute_reply.started":"2023-01-20T12:01:00.848151Z","shell.execute_reply":"2023-01-20T12:01:11.624381Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\naccuracy_score(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:01:28.403724Z","iopub.execute_input":"2023-01-20T12:01:28.405153Z","iopub.status.idle":"2023-01-20T12:03:54.727655Z","shell.execute_reply.started":"2023-01-20T12:01:28.405102Z","shell.execute_reply":"2023-01-20T12:03:54.726597Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0.8055"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n# for random forest model\nconfusion_matrix(y_test,y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:04:59.937709Z","iopub.execute_input":"2023-01-20T12:04:59.938086Z","iopub.status.idle":"2023-01-20T12:04:59.948746Z","shell.execute_reply.started":"2023-01-20T12:04:59.938055Z","shell.execute_reply":"2023-01-20T12:04:59.947790Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"array([[10942,  1684],\n       [ 2206,  5168]])"},"metadata":{}}]},{"cell_type":"markdown","source":"# How to further increase the accuracy\n* Increase the data\n* Preprocessing: Stemming\n* Try more algorithms: SVM, LogisticRegression, Hyperparameter Tuning\n* More Features\n* Try Other Vectorization Method: TFIDF, Word2Vec\n* DeepLearning methods: LSTM, BERT\n","metadata":{}},{"cell_type":"markdown","source":"# Deployment","metadata":{}},{"cell_type":"code","source":"def test_common_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n    return len(w1 & w2)\n\ndef test_total_words(q1,q2):\n    w1 = set(map(lambda word: word.lower().strip(), q1.split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), q2.split(\" \")))    \n    return (len(w1) + len(w2))\n\ndef test_fetch_token_features(q1,q2):\n    \n    SAFE_DIV = 0.0001 \n\n    STOP_WORDS = stopwords.words(\"english\")\n    \n    token_features = [0.0]*8\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n\n    # Get the non-stopwords in Questions\n    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n    \n    #Get the stopwords in Questions\n    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n    \n    # Get the common non-stopwords from Question pair\n    common_word_count = len(q1_words.intersection(q2_words))\n    \n    # Get the common stopwords from Question pair\n    common_stop_count = len(q1_stops.intersection(q2_stops))\n    \n    # Get the common Tokens from Question pair\n    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n    \n    \n    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n    \n    # Last word of both question is same or not\n    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n    \n    # First word of both question is same or not\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    \n    return token_features\n\ndef test_fetch_length_features(q1,q2):\n    \n    length_features = [0.0]*3\n    \n    # Converting the Sentence into Tokens: \n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    \n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return length_features\n    \n    # Absolute length features\n    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n    \n    #Average Token Length of both Questions\n    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n    \n    strs = list(distance.lcsubstrings(q1, q2))\n    length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n    \n    return length_features\n\ndef test_fetch_fuzzy_features(q1,q2):\n    \n    fuzzy_features = [0.0]*4\n    \n    # fuzz_ratio\n    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n    # fuzz_partial_ratio\n    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n    # token_sort_ratio\n    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n    # token_set_ratio\n    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n    return fuzzy_features\n\n","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:05:06.981917Z","iopub.execute_input":"2023-01-20T12:05:06.982267Z","iopub.status.idle":"2023-01-20T12:05:07.001184Z","shell.execute_reply.started":"2023-01-20T12:05:06.982239Z","shell.execute_reply":"2023-01-20T12:05:07.000151Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def query_point_creator(q1,q2):\n    \n    input_query = []\n    \n    # preprocess\n    q1 = preprocess(q1)\n    q2 = preprocess(q2)\n    \n    # fetch basic features\n    input_query.append(len(q1))\n    input_query.append(len(q2))\n    \n    input_query.append(len(q1.split(\" \")))\n    input_query.append(len(q2.split(\" \")))\n    \n    input_query.append(test_common_words(q1,q2))\n    input_query.append(test_total_words(q1,q2))\n    input_query.append(round(test_common_words(q1,q2)/test_total_words(q1,q2),2))\n    \n    # fetch token features\n    token_features = test_fetch_token_features(q1,q2)\n    input_query.extend(token_features)\n    \n    # fetch length based features\n    length_features = test_fetch_length_features(q1,q2)\n    input_query.extend(length_features)\n    \n    # fetch fuzzy features\n    fuzzy_features = test_fetch_fuzzy_features(q1,q2)\n    input_query.extend(fuzzy_features)\n    \n    # bow feature for q1\n    q1_bow = cv.transform([q1]).toarray()\n    \n    # bow feature for q2\n    q2_bow = cv.transform([q2]).toarray()\n    \n    \n    \n    return np.hstack((np.array(input_query).reshape(1,22),q1_bow,q2_bow))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:05:14.525708Z","iopub.execute_input":"2023-01-20T12:05:14.526604Z","iopub.status.idle":"2023-01-20T12:05:14.534801Z","shell.execute_reply.started":"2023-01-20T12:05:14.526552Z","shell.execute_reply":"2023-01-20T12:05:14.533767Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"q1 = 'Where is the capital of India?'\nq2 = 'What is the current capital of Pakistan?'\nq3 = 'Which city serves as the capital of India?'\nq4 = 'What is the business capital of India?'","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:05:17.190690Z","iopub.execute_input":"2023-01-20T12:05:17.191091Z","iopub.status.idle":"2023-01-20T12:05:17.195492Z","shell.execute_reply.started":"2023-01-20T12:05:17.191059Z","shell.execute_reply":"2023-01-20T12:05:17.194759Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"rf.predict(query_point_creator(q1,q4))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:05:31.996723Z","iopub.execute_input":"2023-01-20T12:05:31.997594Z","iopub.status.idle":"2023-01-20T12:05:32.020349Z","shell.execute_reply.started":"2023-01-20T12:05:31.997556Z","shell.execute_reply":"2023-01-20T12:05:32.019314Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"array([0])"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\n\npickle.dump(rf,open('model.pkl','wb'))\npickle.dump(cv,open('cv.pkl','wb'))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:05:38.837419Z","iopub.execute_input":"2023-01-20T12:05:38.837797Z","iopub.status.idle":"2023-01-20T12:05:39.354063Z","shell.execute_reply.started":"2023-01-20T12:05:38.837768Z","shell.execute_reply":"2023-01-20T12:05:39.353036Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"stopwords = stopwords.words(\"english\")","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:14:11.080574Z","iopub.execute_input":"2023-01-20T12:14:11.080976Z","iopub.status.idle":"2023-01-20T12:14:11.086185Z","shell.execute_reply.started":"2023-01-20T12:14:11.080946Z","shell.execute_reply":"2023-01-20T12:14:11.085370Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"pickle.dump(stopwords,open('stopwords.pkl','wb'))","metadata":{"execution":{"iopub.status.busy":"2023-01-20T12:16:29.956803Z","iopub.execute_input":"2023-01-20T12:16:29.957216Z","iopub.status.idle":"2023-01-20T12:16:29.962643Z","shell.execute_reply.started":"2023-01-20T12:16:29.957183Z","shell.execute_reply":"2023-01-20T12:16:29.961591Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### This notebook will be updated in future with the future prospects given above.\n### If you like my work, please upvote.  \n### Thank you","metadata":{}}]}